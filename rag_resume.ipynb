{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-W3fBO7BQpm"
      },
      "source": [
        "# LLM+RAG Resume Q&A Demo\n",
        "\n",
        "[GitHub Repository](https://github.com/cool112624/llm-rag-resume)\n",
        "\n",
        "This notebook lets you upload a resume (PDF or TXT) and interactively ask questions about its content,\n",
        "using Sentence Transformers + FAISS for retrieval and OpenAI GPT-4o mini for answer generation.\n",
        "\n",
        "**No API keys or personal data are stored in this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-8xLzA3ApdW"
      },
      "outputs": [],
      "source": [
        "# 1. Install dependencies\n",
        "!pip install sentence-transformers faiss-cpu openai pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_L2ZqCKBTyo"
      },
      "outputs": [],
      "source": [
        "# 2. Upload your resume (PDF or TXT)\n",
        "print(\"⬆️ Please upload your resume (PDF or TXT) using the button below or by dragging and dropping:\")\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg4Lpz5wBZxa"
      },
      "outputs": [],
      "source": [
        "# 3. Extract text from resume (if PDF)\n",
        "import os\n",
        "resume_file = next(iter(uploaded))\n",
        "resume_text = \"\"\n",
        "if resume_file.lower().endswith('.pdf'):\n",
        "    import pdfplumber\n",
        "    with pdfplumber.open(resume_file) as pdf:\n",
        "        resume_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
        "elif resume_file.lower().endswith('.txt'):\n",
        "    with open(resume_file, encoding=\"utf-8\") as f:\n",
        "        resume_text = f.read()\n",
        "else:\n",
        "    raise ValueError(\"Please upload a PDF or TXT file.\")\n",
        "\n",
        "print(resume_text[:2000])  # Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUZ9tJsgBhnW"
      },
      "outputs": [],
      "source": [
        "# 4. Split resume into chunks (by paragraphs)\n",
        "def parse_resume_by_paragraph(text: str, min_length=40) -> list:\n",
        "    # Split on double newlines and filter short chunks\n",
        "    return [p.strip() for p in text.split('\\n\\n') if len(p.strip()) >= min_length]\n",
        "\n",
        "resume_chunks = parse_resume_by_paragraph(resume_text)\n",
        "print(f\"Parsed {len(resume_chunks)} chunks. Example:\", resume_chunks[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBBjv4amBmY-"
      },
      "outputs": [],
      "source": [
        "# 5. Generate embeddings and build FAISS index\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "doc_embeddings = embedder.encode(resume_chunks)\n",
        "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
        "index.add(np.array(doc_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdzXhxuQ70yH"
      },
      "outputs": [],
      "source": [
        "# 6. API key input (never stored)\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "api_key = getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3H8ULXvBpsS"
      },
      "outputs": [],
      "source": [
        "# 7. Q&A Function\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "def rag_qa(query, top_k=3):\n",
        "    query_emb = embedder.encode([query])\n",
        "    _, I = index.search(np.array(query_emb), top_k)\n",
        "    context = \"\\n\\n\".join([resume_chunks[i] for i in I[0]])\n",
        "    prompt = f\"Given the following context from my resume:\\n\\n{context}\\n\\nAnswer this question: {query}\\n\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZeXwBCjOnFl"
      },
      "outputs": [],
      "source": [
        "# 8. Interactive Q&A Loop\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# Multi-line chat input\n",
        "q_box = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Type your question here (type exit or goodbye to quit)...',\n",
        "    layout=widgets.Layout(width='100%', height='60px')\n",
        ")\n",
        "ask_button = widgets.Button(description=\"Send\", button_style=\"primary\")\n",
        "\n",
        "# Scrollable chat history area\n",
        "chat_output = widgets.Output(layout={\n",
        "    'border': '1px solid #ccc',\n",
        "    'height': '350px',\n",
        "    'overflow_y': 'auto',\n",
        "    'padding': '8px',\n",
        "    'background': '#fafbfc'\n",
        "})\n",
        "chat_history = []\n",
        "\n",
        "def show_chat():\n",
        "    chat_output.clear_output()\n",
        "    with chat_output:\n",
        "        for entry in chat_history:\n",
        "            # User block\n",
        "            display(Markdown(\n",
        "                f\"\"\"<div style=\"background:#f0f4ff;padding:8px 12px;border-radius:8px;margin-bottom:3px;\">\n",
        "                <b>🧑 You:</b><br>{entry['question']}\n",
        "                </div>\"\"\"\n",
        "            ))\n",
        "            display(Markdown(\"<br>\"))\n",
        "            # AI block\n",
        "            display(Markdown(\n",
        "                f\"\"\"<div style=\"background:#f8fff0;padding:8px 12px;border-radius:8px;margin-bottom:14px;\">\n",
        "                <b>🤖 AI:</b><br>{entry['answer']}\n",
        "                </div><br>\"\"\"   # <-- this <br> adds a blank row after AI answer\n",
        "            ))\n",
        "\n",
        "def on_ask_clicked(b):\n",
        "    question = q_box.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "    if question.lower() in ('exit', 'goodbye'):\n",
        "        chat_history.append({'question': question, 'answer': \"Goodbye! Chat ended. Thank you for using LLM-RAG Resume Q&A.\"})\n",
        "        show_chat()\n",
        "        q_box.disabled = True\n",
        "        ask_button.disabled = True\n",
        "        return\n",
        "    answer = rag_qa(question)\n",
        "    chat_history.append({'question': question, 'answer': answer})\n",
        "    show_chat()\n",
        "    q_box.value = ''  # Clear input\n",
        "\n",
        "ask_button.on_click(on_ask_clicked)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>🤖 LLM-RAG Resume Chat</h3><p>Ask anything about your uploaded resume below! Type <b>exit</b> or <b>goodbye</b> to end the chat.</p>\"),\n",
        "    chat_output,\n",
        "    q_box,\n",
        "    ask_button\n",
        "]))\n",
        "\n",
        "show_chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
